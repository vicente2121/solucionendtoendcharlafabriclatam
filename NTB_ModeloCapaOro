# â”€â”€ Extensiones SparkÂ â†”Â Warehouse
import com.microsoft.spark.fabric
from com.microsoft.spark.fabric.Constants import Constants

from datetime import datetime
from pyspark.sql import functions as F, types as T

# Rango del calendario
start_date, end_date = "2025-01-01", "2027-12-31"
num_days = (datetime.strptime(end_date, "%Y-%m-%d")
            - datetime.strptime(start_date, "%Y-%m-%d")).days + 1

# DataFrame de fechas
df_fecha = (spark.range(0, num_days)
            .withColumn("i",      F.col("id").cast("int"))
            .withColumn("fecha",  F.expr(f"date_add('{start_date}', i)"))
            .withColumn("fecha_key",
                        F.date_format("fecha", "yyyyMMdd").cast(T.IntegerType()))
            .withColumn("anio",          F.year("fecha"))
            .withColumn("mes",           F.month("fecha"))
            .withColumn("nombre_mes",    F.date_format("fecha", "MMMM"))
            .withColumn("trimestre",     F.quarter("fecha"))
            .withColumn("semana_iso",    F.weekofyear("fecha"))
            .withColumn("dia_mes",       F.dayofmonth("fecha"))
            .withColumn("dia_sem_iso",   F.dayofweek("fecha"))
            .withColumn("nombre_dia",    F.date_format("fecha", "EEEE"))
            .withColumn("es_fin_semana", F.expr("dia_sem_iso IN (6,7)"))
            .drop("id", "i"))

# Fullâ€‘load al Warehouse
(df_fecha.write
         .mode("overwrite")            # crea la tabla desde cero
         .synapsesql("WH_Oro.dbo.Dim_Fecha"))

print(f"âœ… Dim_Fecha cargada: {df_fecha.count():,} filas "
      f"({start_date}Â â†’Â {end_date}).")


#  SparkÂ â†”Â Warehouse extensions
import com.microsoft.spark.fabric
from com.microsoft.spark.fabric.Constants import Constants

from pyspark.sql import Window, functions as F

# Cargar Silver
silver = spark.read.table("silver_crypto_market")

# DimensiÃ³n Moneda (sÃ³lo lo necesario)
dim_moneda = (silver
              .select("id_moneda", "sigla", "nombre_moneda")
              .distinct())

# Clave surrogate reproducible (alfabÃ©tico por id_moneda)
w = Window.orderBy("id_moneda")
dim_moneda = (dim_moneda
              .withColumn("moneda_id", F.row_number().over(w))
              .select("moneda_id", "id_moneda", "sigla", "nombre_moneda"))

dim_moneda.show(10, truncate=False)

#  FULLâ€‘LOAD al Warehouse
(dim_moneda.write
          .mode("overwrite")
          .synapsesql("WH_Oro.dbo.Dim_Moneda"))

print(f"âœ… Dim_Moneda cargada ({dim_moneda.count()} filas).")


import com.microsoft.spark.fabric
from com.microsoft.spark.fabric.Constants import Constants

from pyspark.sql import Window, functions as F

silver  = spark.read.table("silver_crypto_market")

dim_vs = (silver.select("moneda_vs").distinct())

w_vs = Window.orderBy("moneda_vs")
dim_vs = (dim_vs
          .withColumn("vs_id", F.row_number().over(w_vs))
          .select("vs_id", "moneda_vs"))

dim_vs.show()

(dim_vs.write
       .mode("overwrite")
       .synapsesql("WH_Oro.dbo.Dim_Vs"))

print(f"âœ… Dim_Vs cargada ({dim_vs.count()} filas).")
import com.microsoft.spark.fabric
from com.microsoft.spark.fabric.Constants import Constants

from pyspark.sql import functions as F

# 1 â–¸ cargar materia prima y dimensiones
silver     = spark.read.table("silver_crypto_market")
dim_moneda = spark.read.synapsesql("WH_Oro.dbo.Dim_Moneda")
dim_vs     = spark.read.synapsesql("WH_Oro.dbo.Dim_Vs")

# 2 â–¸ enriquecer y seleccionar columnas finales
f_hecho = (
    silver
    .withColumn(
        "fecha_key",
        F.date_format("fecha_snapshot", "yyyyMMdd").cast("int")
    )
    .join(dim_moneda.select("moneda_id", "id_moneda"), on="id_moneda")
    .join(dim_vs.select("vs_id", "moneda_vs"),           on="moneda_vs")
    .withColumn("fecha_ath", F.to_date("fecha_ath"))      # sÃ³lo fecha
    .select(
        "fecha_key", "moneda_id", "vs_id",
        "precio_actual", "capitalizacion", "volumen_total",
        "variacion_pct_24h", "maximo_24h", "minimo_24h",
        "puesto_market_cap", "max_historico", "variacion_pct_ath",
        "fecha_ath",              # DATE
        "ultima_actualizacion",   # DATETIME â€“ auditorÃ­a
        "ts_ingestion"            # DATETIME â€“ trazabilidad
    )
)

f_hecho.show(10, truncate=False)
print(f"Total filas a escribir: {f_hecho.count():,}")

# 3 â–¸ FULLâ€‘LOAD al Warehouse
(f_hecho.write
       .mode("overwrite")
       .synapsesql("WH_Oro.dbo.F_Hecho_Crypto_Diario"))

print("ðŸŸ¢  F_Hecho_Crypto_Diario cargada en WH_Oro.")
